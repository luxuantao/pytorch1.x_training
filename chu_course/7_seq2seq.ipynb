{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第六课 Seq2Seq, Attention\n",
    "\n",
    "褚则伟 zeweichu@gmail.com\n",
    "\n",
    "在这份notebook当中，我们会(尽可能)复现Luong的attention模型\n",
    "\n",
    "由于我们的数据集非常小，只有一万多个句子的训练数据，所以训练出来的模型效果并不好。如果大家想训练一个好一点的模型，可以参考下面的资料。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更多阅读\n",
    "\n",
    "#### 课件\n",
    "- [cs224d](http://cs224d.stanford.edu/lectures/CS224d-Lecture15.pdf)\n",
    "\n",
    "\n",
    "#### 论文\n",
    "- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n",
    "- [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025?context=cs)\n",
    "- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1406.1078)\n",
    "\n",
    "\n",
    "#### PyTorch代码\n",
    "- [seq2seq-tutorial](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb)\n",
    "- [Tutorial from Ben Trevett](https://github.com/bentrevett/pytorch-seq2seq)\n",
    "- [IBM seq2seq](https://github.com/IBM/pytorch-seq2seq)\n",
    "- [OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)\n",
    "\n",
    "\n",
    "#### 更多关于Machine Translation\n",
    "- [Beam Search](https://www.coursera.org/lecture/nlp-sequence-models/beam-search-4EtHZ)\n",
    "- Pointer network 文本摘要\n",
    "- Copy Mechanism 文本摘要\n",
    "- Converage Loss \n",
    "- ConvSeq2Seq\n",
    "- Transformer\n",
    "- Tensor2Tensor\n",
    "\n",
    "#### TODO\n",
    "- 建议同学尝试对中文进行分词\n",
    "\n",
    "#### NER\n",
    "- https://github.com/allenai/allennlp/tree/master/allennlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读入中英文数据\n",
    "- 英文我们使用nltk的word tokenizer来分词，并且使用小写字母\n",
    "- 中文我们直接使用单个汉字作为基本单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(in_file):\n",
    "    cn = []\n",
    "    en = []\n",
    "    num_examples = 0\n",
    "    with open(in_file, 'r', encoding='UTF-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            en.append([\"BOS\"] + nltk.word_tokenize(line[0].lower()) + [\"EOS\"])\n",
    "            # split chinese sentence into characters\n",
    "            cn.append([\"BOS\"] + [c for c in line[1]] + [\"EOS\"])\n",
    "    return en, cn\n",
    "\n",
    "train_file = \"nmt/en-cn/train.txt\"\n",
    "dev_file = \"nmt/en-cn/dev.txt\"\n",
    "train_en, train_cn = load_data(train_file)\n",
    "dev_en, dev_cn = load_data(dev_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14533, 14533, 1817, 1817)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_en), len(train_en), len(dev_en), len(dev_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BOS', '任', '何', '人', '都', '可', '以', '做', '到', '。', 'EOS']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cn[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建单词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = 0\n",
    "PAD_IDX = 1\n",
    "\n",
    "def build_dict(sentences, max_words=5000):\n",
    "    word_count = Counter()\n",
    "    for sentence in sentences:\n",
    "        for s in sentence:\n",
    "            word_count[s] += 1\n",
    "    ls = word_count.most_common(max_words)\n",
    "    total_words = len(ls) + 2\n",
    "    word_dict = {w[0] : index + 2 for index, w in enumerate(ls)}\n",
    "    word_dict[\"UNK\"] = UNK_IDX\n",
    "    word_dict[\"PAD\"] = PAD_IDX\n",
    "    return word_dict, total_words\n",
    "\n",
    "en_dict, en_total_words = build_dict(train_en)\n",
    "cn_dict, cn_total_words = build_dict(train_cn)\n",
    "inv_en_dict = {v : k for k, v in en_dict.items()}\n",
    "inv_cn_dict = {v : k for k, v in cn_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5002, 3195)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_total_words, cn_total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把单词全部转变成数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(en_sentences, cn_sentences, en_dict, cn_dict, sort_by_len=True):\n",
    "    length = len(en_sentences)\n",
    "    out_en_sentences = [[en_dict.get(w, 0) for w in sent] for sent in en_sentences]\n",
    "    out_cn_sentences = [[cn_dict.get(w, 0) for w in sent] for sent in cn_sentences]\n",
    "\n",
    "    # sort sentences by english lengths\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "       \n",
    "    # 把中文和英文按照同样的顺序排序\n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(out_en_sentences)\n",
    "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
    "        out_cn_sentences = [out_cn_sentences[i] for i in sorted_index]\n",
    "        \n",
    "    return out_en_sentences, out_cn_sentences\n",
    "\n",
    "train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)\n",
    "dev_en, dev_cn = encode(dev_en, dev_cn, en_dict, cn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS 放 松 点 吧 。 EOS\n",
      "BOS just relax . EOS\n"
     ]
    }
   ],
   "source": [
    "k = 101\n",
    "print(\" \".join([inv_cn_dict[i] for i in train_cn[k]]))\n",
    "print(\" \".join([inv_en_dict[i] for i in train_en[k]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把全部句子分成batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatches(n, minibatch_size, shuffle=True):\n",
    "    idx_list = np.arange(0, n, minibatch_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))\n",
    "    return minibatches\n",
    "\n",
    "def prepare_data(seqs):\n",
    "    lengths = [len(seq) for seq in seqs]\n",
    "    n_samples = len(seqs)\n",
    "    max_len = np.max(lengths)\n",
    "    x = np.zeros((n_samples, max_len)).astype('int32')\n",
    "    x_lengths = np.array(lengths).astype(\"int32\")\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        x[idx, :lengths[idx]] = seq\n",
    "    return x, x_lengths #x_mask\n",
    "\n",
    "def gen_examples(en_sentences, cn_sentences, batch_size):\n",
    "    minibatches = get_minibatches(len(en_sentences), batch_size)\n",
    "    all_ex = []\n",
    "    for minibatch in minibatches:\n",
    "        mb_en_sentences = [en_sentences[t] for t in minibatch]\n",
    "        mb_cn_sentences = [cn_sentences[t] for t in minibatch]\n",
    "        mb_x, mb_x_len = prepare_data(mb_en_sentences)\n",
    "        mb_y, mb_y_len = prepare_data(mb_cn_sentences)\n",
    "        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))\n",
    "    return all_ex\n",
    "\n",
    "batch_size = 64\n",
    "train_data = gen_examples(train_en, train_cn, batch_size)\n",
    "random.shuffle(train_data)\n",
    "dev_data = gen_examples(dev_en, dev_cn, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14533, 228)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_en), len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   2,   12, 1102,   15,    6,  347,  117,  173,    4,    3],\n",
       "        [   2,   89,    6,    0,    7,    6, 1797,  697,    4,    3],\n",
       "        [   2,    6,  400,   47,  168,  165,    6,  866,    4,    3],\n",
       "        [   2,   52,   41,    8,   73,    7,   53,   46,   11,    3],\n",
       "        [   2,    5,  211,    8,   63,  187,   23,   28,    4,    3],\n",
       "        [   2,    5,   14,   13,   53,  108,   12,   10,    4,    3],\n",
       "        [   2,   47,    8, 1260,   32,   96,    7,   23,   11,    3],\n",
       "        [   2,   10,   44,  151,    8,   78,   13,   14,   11,    3],\n",
       "        [   2,   51,   93,   57,   49,  597,  212,  218,    4,    3],\n",
       "        [   2,   14,   13,   14,  151,   36,   28,  218,    4,    3],\n",
       "        [   2,    5,  167,   18,   31,   12,   27,  233,    4,    3],\n",
       "        [   2,    5,   84,   25, 1302,  977,   26,  427,    4,    3],\n",
       "        [   2,   19,   10,  329,  124,  103,   65,  103,    4,    3],\n",
       "        [   2,    5,  167,   40,    7,  254,    9,  983,    4,    3],\n",
       "        [   2,   29, 1332,    6, 2475,   17,    6,  615,    4,    3],\n",
       "        [   2,    5,   78,   13,  275,   21,  578,  914,    4,    3],\n",
       "        [   2,    5,  174,    5,   68,  141,   40,  245,    4,    3],\n",
       "        [   2,    5,  283,    9,  491,   15,    6, 1610,    4,    3],\n",
       "        [   2,   14,    8,  302,  195,   15,    6,  549,   11,    3],\n",
       "        [   2,   31,   14,    8,   54,    7,  137,   23,   11,    3],\n",
       "        [   2,    0, 2165,   38,    6,  603,   65,  432,    4,    3],\n",
       "        [   2,  548,   10,  609,   26, 1071,  145, 3374,    4,    3],\n",
       "        [   2,   18,  739,  393,   38,   35, 1025,  119,    4,    3],\n",
       "        [   2,   19,  205,    9, 2244,   26,   40,  371,    4,    3],\n",
       "        [   2,    5,   68,   36,    7,  139,   77, 1409,    4,    3],\n",
       "        [   2,   12, 2268,   99,   38,   35,  135, 1460,    4,    3],\n",
       "        [   2,  181,   15,  310,   28,    8,  136, 1312,    4,    3],\n",
       "        [   2,   25,  896,   10,    9, 4493,   26,    0,    4,    3],\n",
       "        [   2,   19,  337,   52,    7,   14,    6,    0,    4,    3],\n",
       "        [   2,   29,   84,    6,  281,  175,   38,  728,    4,    3],\n",
       "        [   2,    5,   73,  993,  111,  752,  973,   46,    4,    3],\n",
       "        [   2,    5,   27,  176,   26,   82,   25,  173,    4,    3],\n",
       "        [   2,   44,   10,    0,  491,    7,   89,  504,    4,    3],\n",
       "        [   2,   91,   20, 1746,    6,  229,   38,  202,    4,    3],\n",
       "        [   2,   19,  944,    6,   94,    7,    6,  549,    4,    3],\n",
       "        [   2,    5,   79,  746,   21, 4318,   20, 4319,    4,    3],\n",
       "        [   2,   87,   41,    8,   22,   32,  413,  789,   11,    3],\n",
       "        [   2,  414,    8,   26,    0,   25, 1105,  668,    4,    3],\n",
       "        [   2,   12,   10,  329,  124,  483,   65,  483,    4,    3],\n",
       "        [   2,    5,  855,   28,   19,   10,  175,  980,    4,    3],\n",
       "        [   2,  102,   17,    6, 1195,   27,  216,  656,    4,    3],\n",
       "        [   2,    7,   31, 2081,   43,   12,   39,    0,   11,    3],\n",
       "        [   2,   25,  210,   10, 1599,   62,    6, 1120,    4,    3],\n",
       "        [   2,   43,    8, 1137,   25,  610,   26,   23,   11,    3],\n",
       "        [   2,    9, 1350,   27,    0,    7,    6, 1001,    4,    3],\n",
       "        [   2,   28,   10,    6,  422,   87,   12,  841,    4,    3],\n",
       "        [   2,  137,   23,   31,    7,   14,   38,   16,    4,    3],\n",
       "        [   2,   21,  100,   10, 2599,   62,    9,  243,    4,    3],\n",
       "        [   2,    5,   81,   31,    8,  148,   10,  357,    4,    3],\n",
       "        [   2,  143,    5,  425,   38,    9, 1721, 1350,   11,    3],\n",
       "        [   2,   12,   27,  176,   26,    6,    0,  200,    4,    3],\n",
       "        [   2,   22,    8,  302,   83,    7,  138,  687,   11,    3],\n",
       "        [   2,   12, 2118,    9, 2262,   38,   35,  365,    4,    3],\n",
       "        [   2,   19,  144,   23,    9, 1095,   17, 4216,    4,    3],\n",
       "        [   2,    6,    0,  456,    7,   39,   50,  445,    4,    3],\n",
       "        [   2,   19,  167,   46,    7,  270,    6,  443,    4,    3],\n",
       "        [   2,  273,    7, 1259,   25,  177,  112,  173,    4,    3],\n",
       "        [   2,   29,   63,    9, 2209,  217,   26,   40,    4,    3],\n",
       "        [   2,   12,  144,   23,  837,  110,    5, 1408,    4,    3],\n",
       "        [   2,   29,   90,  462,   26,    6,  146,   60,    4,    3],\n",
       "        [   2,    5,   42,   37,   69,    7,  258,   46,    4,    3],\n",
       "        [   2,  303,  155,   10,  128,  135,    7,  254,    4,    3],\n",
       "        [   2,   31,   90,   32, 1366,   26,    6,  688,   11,    3],\n",
       "        [   2,   51,  144,    9,  221,  217,   26,   23,    4,    3]]),\n",
       " array([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]),\n",
       " array([[  2,   9, 128, ...,   0,   0,   0],\n",
       "        [  2, 572, 645, ...,   0,   0,   0],\n",
       "        [  2, 411, 579, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [  2, 373,  80, ...,   0,   0,   0],\n",
       "        [  2,   8, 393, ...,   0,   0,   0],\n",
       "        [  2,   9,  24, ...,   3,   0,   0]]),\n",
       " array([14, 10, 12, 11, 13, 10, 14, 11, 17, 13, 12, 15, 11, 10, 10, 15, 12,\n",
       "        11, 13, 10, 15, 10, 14, 15, 13, 17, 11, 14, 12, 15, 18, 12, 11, 16,\n",
       "        12, 15, 10, 12, 11, 12, 18, 13, 12, 13, 14, 12, 11,  9, 12, 12, 14,\n",
       "        12, 13, 14, 11, 11, 14, 15, 15, 11,  9, 13, 11, 16]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 10), (64,), (64, 18), (64,))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[0][0].shape, dev_data[0][1].shape, dev_data[0][2].shape, dev_data[0][3].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 没有Attention的版本\n",
    "下面是一个更简单的没有Attention的encoder decoder模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        super(PlainEncoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, lengths): # x(bs, seq_len) lengths(bs,)\n",
    "        # 把一个batch里面的seq按长度从大到小排序\n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
    "        x_sorted = x[sorted_idx.long()]\n",
    "        embedded = self.dropout(self.embed(x_sorted)) # (bs, seq_len, embedding_size)\n",
    "        # 处理一个batch里seq不等长的问题\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        packed_out, hid = self.rnn(packed_embedded) # hid: torch.Size([1, 64, 100])\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True) # out: torch.Size([64, 10, 100])\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        out = out[original_idx.long()].contiguous() # (batch_size, seq_len, num_directions * hidden_size)\n",
    "        hid = hid[:, original_idx.long()].contiguous() # num_layers * num_directions, batch_size, hidden_size)\n",
    "        return out, hid[[-1]] # (1, bs, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        super(PlainDecoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, y, y_lengths, hid):\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]\n",
    "        hid = hid[:, sorted_idx.long()]\n",
    "        y_sorted = self.dropout(self.embed(y_sorted)) # batch_size, output_length, embed_size\n",
    "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        out, hid = self.rnn(packed_seq, hid)\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        output_seq = unpacked[original_idx.long()].contiguous() # (batch_size, seq_len, num_directions * hidden_size)\n",
    "        hid = hid[:, original_idx.long()].contiguous() # (1, bs, hidden_size)\n",
    "        output = F.log_softmax(self.out(output_seq), -1)  # (batch_size, seq_len, vocab_size)\n",
    "        return output, hid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(PlainSeq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        output, hid = self.decoder(y=y, y_lengths=y_lengths, hid=hid)\n",
    "        return output, None\n",
    "    \n",
    "    def translate(self, x, x_lengths, y, max_length=10):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths) # hid (1, bs, hidden_size)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for i in range(max_length):\n",
    "            output, hid = self.decoder(y=y, y_lengths=torch.ones(batch_size).long().to(y.device), hid=hid)\n",
    "            # output (batch_size, seq_len=1, vocab_size)\n",
    "            y = output.max(2)[1].view(batch_size, 1) # (batch_size, seq_len=1)\n",
    "            preds.append(y)\n",
    "        return torch.cat(preds, 1), None # (batch_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked cross entropy loss\n",
    "class LanguageModelCriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LanguageModelCriterion, self).__init__()\n",
    "\n",
    "    def forward(self, input, target, mask): # mask (bs, max_len)\n",
    "        input = input.contiguous().view(-1, input.size(2)) # input: (bs*(seq_len-1),  vocab_size)\n",
    "        target = target.contiguous().view(-1, 1) # (bs*(seq_len-1), 1)\n",
    "        mask = mask.contiguous().view(-1, 1) # mask (bs*max_len, 1)\n",
    "        output = -input.gather(1, target) * mask\n",
    "        output = torch.sum(output) / torch.sum(mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dropout = 0.2\n",
    "hidden_size = 100\n",
    "encoder = PlainEncoder(vocab_size=en_total_words,\n",
    "                      hidden_size=hidden_size,\n",
    "                      dropout=dropout)\n",
    "decoder = PlainDecoder(vocab_size=cn_total_words,\n",
    "                      hidden_size=hidden_size,\n",
    "                      dropout=dropout)\n",
    "model = PlainSeq2Seq(encoder, decoder)\n",
    "model.to(device)\n",
    "loss_fn = LanguageModelCriterion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    total_num_words = total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "            mb_y_len[mb_y_len<=0] = 1\n",
    "\n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "\n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "\n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "\n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "            \n",
    "    print(\"Evaluation loss\", total_loss/total_num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iteration 0 loss 3.4940872192382812\n",
      "Epoch 0 iteration 100 loss 2.6657497882843018\n",
      "Epoch 0 iteration 200 loss 4.222717761993408\n",
      "Epoch 0 Training loss 2.9568770158206696\n",
      "Evaluation loss 3.32023454957307\n",
      "Epoch 1 iteration 0 loss 3.4440457820892334\n",
      "Epoch 1 iteration 100 loss 2.6093668937683105\n",
      "Epoch 1 iteration 200 loss 4.169187545776367\n",
      "Epoch 1 Training loss 2.898792819107096\n",
      "Epoch 2 iteration 0 loss 3.398470401763916\n",
      "Epoch 2 iteration 100 loss 2.5461716651916504\n",
      "Epoch 2 iteration 200 loss 4.138885021209717\n",
      "Epoch 2 Training loss 2.844290894448462\n",
      "Epoch 3 iteration 0 loss 3.383193254470825\n",
      "Epoch 3 iteration 100 loss 2.489271402359009\n",
      "Epoch 3 iteration 200 loss 4.115822792053223\n",
      "Epoch 3 Training loss 2.792332657922521\n",
      "Epoch 4 iteration 0 loss 3.3360700607299805\n",
      "Epoch 4 iteration 100 loss 2.445664882659912\n",
      "Epoch 4 iteration 200 loss 4.084988117218018\n",
      "Epoch 4 Training loss 2.747594583657235\n",
      "Epoch 5 iteration 0 loss 3.2986810207366943\n",
      "Epoch 5 iteration 100 loss 2.373203992843628\n",
      "Epoch 5 iteration 200 loss 4.048251628875732\n",
      "Epoch 5 Training loss 2.700184222838607\n",
      "Evaluation loss 3.2243015601503195\n",
      "Epoch 6 iteration 0 loss 3.240203619003296\n",
      "Epoch 6 iteration 100 loss 2.3635079860687256\n",
      "Epoch 6 iteration 200 loss 4.042222023010254\n",
      "Epoch 6 Training loss 2.6623820576331654\n",
      "Epoch 7 iteration 0 loss 3.192133665084839\n",
      "Epoch 7 iteration 100 loss 2.299370288848877\n",
      "Epoch 7 iteration 200 loss 3.984339952468872\n",
      "Epoch 7 Training loss 2.6224232017042923\n",
      "Epoch 8 iteration 0 loss 3.203190565109253\n",
      "Epoch 8 iteration 100 loss 2.2668113708496094\n",
      "Epoch 8 iteration 200 loss 3.98203444480896\n",
      "Epoch 8 Training loss 2.5824434752050824\n",
      "Epoch 9 iteration 0 loss 3.1537694931030273\n",
      "Epoch 9 iteration 100 loss 2.253316879272461\n",
      "Epoch 9 iteration 200 loss 3.9454116821289062\n",
      "Epoch 9 Training loss 2.5532969745482244\n"
     ]
    }
   ],
   "source": [
    "def train(model, data, num_epochs=20):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_num_words = total_loss = 0.\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data): # mb_y (bs, seq_len)\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long() # (bs, seq_len-1)\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long() # (bs, seq_len-1)\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long() # mb_y_len (bs,)\n",
    "            mb_y_len[mb_y_len<=0] = 1\n",
    "            \n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "            \n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None] \n",
    "            mb_out_mask = mb_out_mask.float() # (bs, max_len)\n",
    "            \n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask) # 平均值\n",
    "            \n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "            \n",
    "            # 更新模型\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if it % 100 == 0:\n",
    "                print(\"Epoch\", epoch, \"iteration\", it, \"loss\", loss.item())\n",
    "                \n",
    "        print(\"Epoch\", epoch, \"Training loss\", total_loss / total_num_words)\n",
    "        if epoch % 5 == 0:\n",
    "            evaluate(model, dev_data)\n",
    "train(model, train_data, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS you have nice skin . EOS\n",
      "BOS 你 的 皮 膚 真 好 。 EOS\n",
      "你有人吃饭。\n",
      "\n",
      "BOS you 're UNK correct . EOS\n",
      "BOS 你 部 分 正 确 。 EOS\n",
      "你是个好的。\n",
      "\n",
      "BOS everyone UNK his courage . EOS\n",
      "BOS 每 個 人 都 佩 服 他 的 勇 氣 。 EOS\n",
      "每個人都知道他的房子\n",
      "\n",
      "BOS what time is it ? EOS\n",
      "BOS 几 点 了 ？ EOS\n",
      "它是什麼？\n",
      "\n",
      "BOS i 'm free tonight . EOS\n",
      "BOS 我 今 晚 有 空 。 EOS\n",
      "我昨天得很好。\n",
      "\n",
      "BOS here is your book . EOS\n",
      "BOS 這 是 你 的 書 。 EOS\n",
      "这是你的工作。\n",
      "\n",
      "BOS they are at lunch . EOS\n",
      "BOS 他 们 在 吃 午 饭 。 EOS\n",
      "他們在學校。\n",
      "\n",
      "BOS this chair is UNK . EOS\n",
      "BOS 這 把 椅 子 很 UNK 。 EOS\n",
      "這是一個好的人。\n",
      "\n",
      "BOS it 's pretty heavy . EOS\n",
      "BOS 它 真 重 。 EOS\n",
      "它是我們的。\n",
      "\n",
      "BOS many attended his funeral . EOS\n",
      "BOS 很 多 人 都 参 加 了 他 的 葬 礼 。 EOS\n",
      "把他的房間裡有一個小\n",
      "\n",
      "BOS training will be provided . EOS\n",
      "BOS 会 有 训 练 。 EOS\n",
      "一點會來。\n",
      "\n",
      "BOS someone is watching you . EOS\n",
      "BOS 有 人 在 看 著 你 。 EOS\n",
      "有人在你的眼睛。\n",
      "\n",
      "BOS i slapped his face . EOS\n",
      "BOS 我 摑 了 他 的 臉 。 EOS\n",
      "我很忙。\n",
      "\n",
      "BOS i like UNK music . EOS\n",
      "BOS 我 喜 歡 流 行 音 樂 。 EOS\n",
      "我喜歡爵士頓。\n",
      "\n",
      "BOS tom had no children . EOS\n",
      "BOS T o m 沒 有 孩 子 。 EOS\n",
      "汤姆不是不同的。\n",
      "\n",
      "BOS please lock the door . EOS\n",
      "BOS 請 把 門 鎖 上 。 EOS\n",
      "请关门。\n",
      "\n",
      "BOS tom has calmed down . EOS\n",
      "BOS 汤 姆 冷 静 下 来 了 。 EOS\n",
      "汤姆在学习。\n",
      "\n",
      "BOS please speak more loudly . EOS\n",
      "BOS 請 說 大 聲 一 點 兒 。 EOS\n",
      "請把窗戶。\n",
      "\n",
      "BOS keep next sunday free . EOS\n",
      "BOS 把 下 周 日 空 出 来 。 EOS\n",
      "去年夏天去上班。\n",
      "\n",
      "BOS i made a mistake . EOS\n",
      "BOS 我 犯 了 一 個 錯 。 EOS\n",
      "我一個很好的。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def translate_dev(i):\n",
    "    en_sent = \" \".join([inv_en_dict[w] for w in dev_en[i]])\n",
    "    print(en_sent)\n",
    "    cn_sent = \" \".join([inv_cn_dict[w] for w in dev_cn[i]])\n",
    "    print(\"\".join(cn_sent))\n",
    "\n",
    "    mb_x = torch.from_numpy(np.array(dev_en[i]).reshape(1, -1)).long().to(device)\n",
    "    mb_x_len = torch.from_numpy(np.array([len(dev_en[i])])).long().to(device)\n",
    "    bos = torch.Tensor([[cn_dict[\"BOS\"]]]).long().to(device)\n",
    "\n",
    "    translation, attn = model.translate(mb_x, mb_x_len, bos)\n",
    "    translation = [inv_cn_dict[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
    "    trans = []\n",
    "    for word in translation:\n",
    "        if word != \"EOS\":\n",
    "            trans.append(word)\n",
    "        else:\n",
    "            break\n",
    "    print(\"\".join(trans))\n",
    "\n",
    "for i in range(100,120):\n",
    "    translate_dev(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据全部处理完成，现在我们开始构建seq2seq模型\n",
    "\n",
    "#### Encoder\n",
    "- Encoder模型的任务是把输入文字传入embedding层和GRU层，转换成一些hidden states作为后续的context vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, enc_hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(enc_hidden_size * 2, dec_hidden_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
    "        x_sorted = x[sorted_idx.long()]\n",
    "        embedded = self.dropout(self.embed(x_sorted))\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        packed_out, hid = self.rnn(packed_embedded)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        out = out[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous() # (2, bs, hidden_size)\n",
    "        \n",
    "        hid = torch.cat([hid[-2], hid[-1]], dim=1) # (bs, hidden_size*2)\n",
    "        hid = torch.tanh(self.fc(hid)).unsqueeze(0) # (1, bs, hidden_size*2)\n",
    "\n",
    "        return out, hid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Luong Attention\n",
    "- 根据context vectors和当前的输出hidden states，计算输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hidden_size, dec_hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "        self.dec_hidden_size = dec_hidden_size\n",
    "        self.linear_in = nn.Linear(enc_hidden_size*2, dec_hidden_size, bias=False)\n",
    "        self.linear_out = nn.Linear(enc_hidden_size*2 + dec_hidden_size, dec_hidden_size)\n",
    "        \n",
    "    def forward(self, output, context, mask):\n",
    "        # output: batch_size, output_len, dec_hidden_size\n",
    "        # context: batch_size, context_len, 2*enc_hidden_size\n",
    "        batch_size = output.size(0)\n",
    "        output_len = output.size(1)\n",
    "        input_len = context.size(1)\n",
    "        \n",
    "        context_in = self.linear_in(context) # batch_size, context_len, dec_hidden_size\n",
    "        \n",
    "        # context_in.transpose(1,2): batch_size, dec_hidden_size, context_len \n",
    "        # output: batch_size, output_len, dec_hidden_size\n",
    "        attn = torch.bmm(output, context_in.transpose(1,2))  # batch_size, output_len, context_len\n",
    "        attn.data.masked_fill(mask, -1e6)\n",
    "        attn = F.softmax(attn, dim=2) # batch_size, output_len, context_len\n",
    "        context = torch.bmm(attn, context) # batch_size, output_len, 2*enc_hidden_size\n",
    "        output = torch.cat((context, output), dim=2) # batch_size, output_len, 2*enc_hidden_size+dec_hidden_size\n",
    "        output = torch.tanh(self.linear_out(output)) # batch_size, output_len, dec_hidden_size\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder\n",
    "- decoder会根据已经翻译的句子内容，和context vectors，来决定下一个输出的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = Attention(enc_hidden_size, dec_hidden_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(dec_hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def create_mask(self, x_len, y_len):\n",
    "        # a mask of shape x_len * y_len\n",
    "        device = x_len.device\n",
    "        max_x_len = x_len.max()\n",
    "        max_y_len = y_len.max()\n",
    "        x_mask = torch.arange(max_x_len, device=x_len.device)[None, :] < x_len[:, None] # (bs, max_x_len)\n",
    "        y_mask = torch.arange(max_y_len, device=x_len.device)[None, :] < y_len[:, None] # (bs, max_y_len)\n",
    "        mask = (x_mask[:, :, None] * y_mask[:, None, :]).type(torch.bool)\n",
    "        mask = ~mask \n",
    "        return mask\n",
    "    \n",
    "    def forward(self, ctx, ctx_lengths, y, y_lengths, hid):\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]\n",
    "        hid = hid[:, sorted_idx.long()]\n",
    "        y_sorted = self.dropout(self.embed(y_sorted)) # batch_size, output_length, embed_size\n",
    "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        out, hid = self.rnn(packed_seq, hid)\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        output_seq = unpacked[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "\n",
    "        mask = self.create_mask(y_lengths, ctx_lengths)\n",
    "        output, attn = self.attention(output_seq, ctx, mask)\n",
    "        output = F.log_softmax(self.out(output), -1)\n",
    "        return output, hid, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq2Seq\n",
    "- 最后我们构建Seq2Seq模型把encoder, attention, decoder串到一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        output, hid, attn = self.decoder(ctx=encoder_out, \n",
    "                    ctx_lengths=x_lengths,\n",
    "                    y=y,\n",
    "                    y_lengths=y_lengths,\n",
    "                    hid=hid)\n",
    "        return output, attn\n",
    "    \n",
    "    def translate(self, x, x_lengths, y, max_length=100):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for i in range(max_length):\n",
    "            output, hid, attn = self.decoder(ctx=encoder_out, \n",
    "                    ctx_lengths=x_lengths,\n",
    "                    y=y,\n",
    "                    y_lengths=torch.ones(batch_size).long().to(y.device),\n",
    "                    hid=hid)\n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "            preds.append(y)\n",
    "            attns.append(attn)\n",
    "        return torch.cat(preds, 1), torch.cat(attns, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.2\n",
    "embed_size = hidden_size = 100\n",
    "encoder = Encoder(vocab_size=en_total_words,\n",
    "                       embed_size=embed_size,\n",
    "                      enc_hidden_size=hidden_size,\n",
    "                       dec_hidden_size=hidden_size,\n",
    "                      dropout=dropout)\n",
    "decoder = Decoder(vocab_size=cn_total_words,\n",
    "                      embed_size=embed_size,\n",
    "                      enc_hidden_size=hidden_size,\n",
    "                       dec_hidden_size=hidden_size,\n",
    "                      dropout=dropout)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "model.to(device)\n",
    "loss_fn = LanguageModelCriterion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iteration 0 loss 8.080053329467773\n",
      "Epoch 0 iteration 100 loss 5.108041763305664\n",
      "Epoch 0 iteration 200 loss 6.071203708648682\n",
      "Epoch 0 Training loss 5.471942504316919\n",
      "Evaluation loss 5.009979196890252\n",
      "Epoch 1 iteration 0 loss 5.340686321258545\n",
      "Epoch 1 iteration 100 loss 4.590888500213623\n",
      "Epoch 1 iteration 200 loss 5.688087463378906\n",
      "Epoch 1 Training loss 4.834882560908583\n",
      "Epoch 2 iteration 0 loss 4.9869771003723145\n",
      "Epoch 2 iteration 100 loss 4.193244457244873\n",
      "Epoch 2 iteration 200 loss 5.386521339416504\n",
      "Epoch 2 Training loss 4.46770390389915\n",
      "Epoch 3 iteration 0 loss 4.68386173248291\n",
      "Epoch 3 iteration 100 loss 3.874641180038452\n",
      "Epoch 3 iteration 200 loss 5.14094877243042\n",
      "Epoch 3 Training loss 4.161714346247877\n",
      "Epoch 4 iteration 0 loss 4.415943622589111\n",
      "Epoch 4 iteration 100 loss 3.5901010036468506\n",
      "Epoch 4 iteration 200 loss 4.927552223205566\n",
      "Epoch 4 Training loss 3.9029811815435664\n",
      "Epoch 5 iteration 0 loss 4.194650650024414\n",
      "Epoch 5 iteration 100 loss 3.3818845748901367\n",
      "Epoch 5 iteration 200 loss 4.728575706481934\n",
      "Epoch 5 Training loss 3.680054182762051\n",
      "Evaluation loss 3.6742580865077206\n",
      "Epoch 6 iteration 0 loss 4.0367631912231445\n",
      "Epoch 6 iteration 100 loss 3.2278239727020264\n",
      "Epoch 6 iteration 200 loss 4.568161964416504\n",
      "Epoch 6 Training loss 3.4858426754476257\n",
      "Epoch 7 iteration 0 loss 3.852928400039673\n",
      "Epoch 7 iteration 100 loss 3.0332438945770264\n",
      "Epoch 7 iteration 200 loss 4.467926502227783\n",
      "Epoch 7 Training loss 3.3152482294132337\n",
      "Epoch 8 iteration 0 loss 3.739957094192505\n",
      "Epoch 8 iteration 100 loss 2.881983995437622\n",
      "Epoch 8 iteration 200 loss 4.355915546417236\n",
      "Epoch 8 Training loss 3.1626530110472104\n",
      "Epoch 9 iteration 0 loss 3.596637725830078\n",
      "Epoch 9 iteration 100 loss 2.7401235103607178\n",
      "Epoch 9 iteration 200 loss 4.268263339996338\n",
      "Epoch 9 Training loss 3.028044221826485\n"
     ]
    }
   ],
   "source": [
    "train(model, train_data, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS you have nice skin . EOS\n",
      "BOS 你 的 皮 膚 真 好 。 EOS\n",
      "你有很多意思。\n",
      "\n",
      "BOS you 're UNK correct . EOS\n",
      "BOS 你 部 分 正 确 。 EOS\n",
      "你是美国的。\n",
      "\n",
      "BOS everyone UNK his courage . EOS\n",
      "BOS 每 個 人 都 佩 服 他 的 勇 氣 。 EOS\n",
      "每個人都在他的手錶。\n",
      "\n",
      "BOS what time is it ? EOS\n",
      "BOS 几 点 了 ？ EOS\n",
      "它是什么？\n",
      "\n",
      "BOS i 'm free tonight . EOS\n",
      "BOS 我 今 晚 有 空 。 EOS\n",
      "我昨天了。\n",
      "\n",
      "BOS here is your book . EOS\n",
      "BOS 這 是 你 的 書 。 EOS\n",
      "这是你的書。\n",
      "\n",
      "BOS they are at lunch . EOS\n",
      "BOS 他 们 在 吃 午 饭 。 EOS\n",
      "他們是個人都在。\n",
      "\n",
      "BOS this chair is UNK . EOS\n",
      "BOS 這 把 椅 子 很 UNK 。 EOS\n",
      "這是这个男孩。\n",
      "\n",
      "BOS it 's pretty heavy . EOS\n",
      "BOS 它 真 重 。 EOS\n",
      "它是不是的。\n",
      "\n",
      "BOS many attended his funeral . EOS\n",
      "BOS 很 多 人 都 参 加 了 他 的 葬 礼 。 EOS\n",
      "他的房子是他的意思。\n",
      "\n",
      "BOS training will be provided . EOS\n",
      "BOS 会 有 训 练 。 EOS\n",
      "那個人都會在家裡。\n",
      "\n",
      "BOS someone is watching you . EOS\n",
      "BOS 有 人 在 看 著 你 。 EOS\n",
      "有人你的人。\n",
      "\n",
      "BOS i slapped his face . EOS\n",
      "BOS 我 摑 了 他 的 臉 。 EOS\n",
      "我把他的手錶。\n",
      "\n",
      "BOS i like UNK music . EOS\n",
      "BOS 我 喜 歡 流 行 音 樂 。 EOS\n",
      "我喜歡打網球。\n",
      "\n",
      "BOS tom had no children . EOS\n",
      "BOS T o m 沒 有 孩 子 。 EOS\n",
      "汤姆没有孩子。\n",
      "\n",
      "BOS please lock the door . EOS\n",
      "BOS 請 把 門 鎖 上 。 EOS\n",
      "請把鹽遞打電話。\n",
      "\n",
      "BOS tom has calmed down . EOS\n",
      "BOS 汤 姆 冷 静 下 来 了 。 EOS\n",
      "汤姆有空子。\n",
      "\n",
      "BOS please speak more loudly . EOS\n",
      "BOS 請 說 大 聲 一 點 兒 。 EOS\n",
      "請快樂。\n",
      "\n",
      "BOS keep next sunday free . EOS\n",
      "BOS 把 下 周 日 空 出 来 。 EOS\n",
      "從週在学习。\n",
      "\n",
      "BOS i made a mistake . EOS\n",
      "BOS 我 犯 了 一 個 錯 。 EOS\n",
      "我是一個人的人。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(100,120):\n",
    "    translate_dev(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

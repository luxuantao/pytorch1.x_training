{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tud\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(1)\n",
    "\n",
    "C = 3\n",
    "K = 100\n",
    "NUM_EPOCHS = 2\n",
    "MAX_VACB_SIZE = 30000\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.2\n",
    "EMBEDDING_SIZE = 100\n",
    "\n",
    "def word_tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text8/text8.train.txt') as fin:\n",
    "    text = fin.read()\n",
    "    \n",
    "text = text.split()\n",
    "vocab = dict(Counter(text).most_common(MAX_VACB_SIZE - 1))\n",
    "vocab['<unk>'] = len(text) - np.sum(list(vocab.values()))\n",
    "\n",
    "idx_to_word = [word for word in vocab.keys()]\n",
    "word_to_idx = {word:i for i, word in enumerate(idx_to_word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0),\n",
       " ('of', 1),\n",
       " ('and', 2),\n",
       " ('one', 3),\n",
       " ('in', 4),\n",
       " ('a', 5),\n",
       " ('to', 6),\n",
       " ('zero', 7),\n",
       " ('nine', 8),\n",
       " ('two', 9)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_to_idx.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = np.array([count for count in vocab.values()], dtype=np.float32)\n",
    "word_freqs = word_counts / np.sum(word_counts)\n",
    "word_freqs = word_freqs ** (3./4.)\n",
    "word_freqs = word_freqs / np.sum(word_freqs)\n",
    "VOCAB_SIZE = len(idx_to_word)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingDataset(tud.Dataset):\n",
    "    def __init__(self, text, word_to_idx, idx_to_word, word_freqs, word_counts):\n",
    "        super(WordEmbeddingDataset, self).__init__()\n",
    "        self.text_encoded = [word_to_idx.get(word, word_to_idx['<unk>']) for word in text]\n",
    "        self.text_encoded = torch.LongTensor(self.text_encoded)\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = idx_to_word\n",
    "        self.word_freqs = torch.Tensor(word_freqs)\n",
    "        self.word_counts = torch.Tensor(word_counts)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text_encoded)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        center_word = self.text_encoded[idx]\n",
    "        pos_indices = list(range(idx-C, idx)) + list(range(idx+1, idx+C+1))\n",
    "        pos_indices = [i % len(self.text_encoded) for i in pos_indices]\n",
    "        pos_words = self.text_encoded[pos_indices]\n",
    "        neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[0], True)\n",
    "        return center_word, pos_words, neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WordEmbeddingDataset(text, word_to_idx, idx_to_word, word_freqs, word_counts)\n",
    "dataloader = tud.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([  819,    45,   621,    15,  1797,    29,   328,   157,    25,   598,\n",
       "             9,    13,    25,    12,     5,  4344,     3,    13,     0,     5,\n",
       "          1532,   648,     9,   937,    16, 22599,    85,  7406,  2801,   419,\n",
       "          1238,     1,   966,  1655,   644,     6,    16, 18573, 11226,    37,\n",
       "           261,  1514,  3537,     1, 29999,   644,     4,   210,   110,     5,\n",
       "          3316,  1454,    29,     7,     0,   825,     2,  3992,  2991,  9029,\n",
       "          1881,     0, 20161,    13,     5,     4, 12028,  7117,   394,     3,\n",
       "         27580,  3642,    36,  2050,    92,     8, 23976,  2184,   335,   339,\n",
       "          1314,    15,    34,   284,  4247,  2389,    25,  8552,     0,  1467,\n",
       "           131,  5437,     1, 10596,     2,     4,  1963,    37,     5,   401,\n",
       "          2111,     2,     6,     0,    14,    10,     1,     9,  5363, 12439,\n",
       "          8464,     1,   432,   298,  4171, 11035,     0,  3513,     4,   969,\n",
       "            28,   836, 29999,    88,     5,     6,    63,     0]),\n",
       " tensor([[ 2272,    27,     0,  2606,     1,    47],\n",
       "         [    0,  4499,   317,  1058,   102, 22988],\n",
       "         [   32,   444,     0,    14,  4927,    10],\n",
       "         [ 1163,    40,   564,     1,     0,    22],\n",
       "         [   46,  1981,  1245,   674,    79,   110],\n",
       "         [12472,    35,   748,    57,   955,  3884],\n",
       "         [ 1694,    39,   786,  6889,  1990,   612],\n",
       "         [    4,     0,   250,    56,   319,  2668],\n",
       "         [    5, 29999,     2,    36,   288,  1021],\n",
       "         [ 4031,     4,     0, 11759,     1,    32],\n",
       "         [  684,    16,    12,   683,    15,    15],\n",
       "         [   17,   110,    86,    29,  1736,    66],\n",
       "         [29999,   847,    58,     9,  1051, 24288],\n",
       "         [    5,    75,     8,   153,    24,     0],\n",
       "         [ 9697,  3159,    10,  3159,   179,  1570],\n",
       "         [  211,   338,   260,    27,     0,   468],\n",
       "         [ 3873,     3,    16,    21,   798,     9],\n",
       "         [    8,    12,    21,    44,  3424,    35],\n",
       "         [    1,    96,    10, 11515,     1,    96],\n",
       "         [  244,   489,  5607,  2193,  8585,   292],\n",
       "         [   26,  2866,    13,     6,   749,   673],\n",
       "         [    8,    21,    12, 25623,   338,  5095],\n",
       "         [   28,   647,    66,    20,     7,   885],\n",
       "         [ 9385,    34,     0,   198,     1,     0],\n",
       "         [  757,    11,     5,  1639,    28,    20],\n",
       "         [  142,     1,     5,  7680,  1882,  2602],\n",
       "         [    0,   637,   237,     0,    84,    40],\n",
       "         [   28,     5,   353,    33,    68,    31],\n",
       "         [  122,     5,   144, 11340,  1005,  3618],\n",
       "         [ 2505, 29999,     6,    15,     7,  6395],\n",
       "         [    1,     0,   172,  3931,    17,     5],\n",
       "         [ 3354,     1,  6589,   157,    24,   869],\n",
       "         [ 4417,     6,  8448,   483,     3,    12],\n",
       "         [ 1574,     2,     5,     1,    42,   233],\n",
       "         [  953,     4,     0,   150,  2127,    97],\n",
       "         [   50,   884,   748,     0,   193,   334],\n",
       "         [    7,     7,    21,   299,     6,  5642],\n",
       "         [    2,     0,   224,  1287,   180,    26],\n",
       "         [  562,  6402,    32,  4134,     6,    42],\n",
       "         [    2, 15414,    56, 27786,   733,     1],\n",
       "         [ 2202,   335,     2,     0,  3290, 17330],\n",
       "         [25288,     0,   646,  1171,  1966,     4],\n",
       "         [  213,  3453,     2,     1,   274,  4988],\n",
       "         [ 5747,     0, 29999,  3981,    71,  6405],\n",
       "         [29999,  2623,  4731,   469,   443,   988],\n",
       "         [    8,     8,    12,  4941,    40,    65],\n",
       "         [  121,     1,  3548,     9,    20,   146],\n",
       "         [  409,    14,  1704,     0,  1030,   428],\n",
       "         [ 1786,  3268,    11,    11,     0,   319],\n",
       "         [ 6851,   777,    97,   112,     1, 28935],\n",
       "         [ 5260,  8038,     5,     4,  5599,    14],\n",
       "         [  679,   263,    36,    27,    44, 24014],\n",
       "         [    4,  1239,    13,   137,    24,  1741],\n",
       "         [   15,     7,     7,     7,    82,     0],\n",
       "         [ 1794,  2762,     4,   266,  1959,    18],\n",
       "         [  234,     1,     0,    23,     0,   646],\n",
       "         [ 2307,    11,  1601, 29999,    47,    62],\n",
       "         [22603,  6322,    56,     0,    66,   443],\n",
       "         [18717,     1,     0,  7223,  8807,   149],\n",
       "         [    6,  6319,  7899,  2791,     4,     0],\n",
       "         [ 5459,     1,     0,     5,   225,    10],\n",
       "         [ 1068,   349,    60,  2656,  4020,    11],\n",
       "         [  273,    31,    65, 21506,    24,     0],\n",
       "         [ 9238,   241,   422,  1780,  1072, 29999],\n",
       "         [   11,    13,   101, 18473,    13,  3665],\n",
       "         [    0,   295,  1565,    50,    16,  3322],\n",
       "         [    1,     0,   222, 10240,    38,    21],\n",
       "         [  127,  3155,     1,    11,     5,   357],\n",
       "         [25411,    58,  1357,     6,   842,  1643],\n",
       "         [   46,    62,     4,     8,     3,     7],\n",
       "         [  783,     1,     0,   211,    33, 24771],\n",
       "         [   19,   653,     0,     0,   203,     2],\n",
       "         [  325,   432,     2,    13,    30,   258],\n",
       "         [    6,    31,  3117,   812,     0,   297],\n",
       "         [  215,    64,  2863,  5528,    27,     5],\n",
       "         [    8,    16,     3,     3,    15, 23595],\n",
       "         [  465,   164,  1327,   285,  5843,  7163],\n",
       "         [    6,   501,    18,     4,     3,     8],\n",
       "         [ 5006,    39,  8418,     6,     0,   113],\n",
       "         [   88,    26,     0,  5657,   585,  5292],\n",
       "         [  103,    70,   129,     5,    57,  1557],\n",
       "         [27242,    34,    12,     8,    12,   129],\n",
       "         [ 1402, 18285, 19254,     5,  2529,  1852],\n",
       "         [ 3695,   427,     4, 29999,  3561,    28],\n",
       "         [ 6924,  9775,    25,    18,     0, 20658],\n",
       "         [ 1057,  9531,  2467,  3940,   792,  7890],\n",
       "         [ 1194, 29999,    58,    37,  5409, 12948],\n",
       "         [  470,  1812,  1812, 29999,  2921,    13],\n",
       "         [   21,     3,     1,  1981,     1,   565],\n",
       "         [  843, 29999,  4849,    23,   306,  1630],\n",
       "         [   39,    36,     0,    11,   157,     1],\n",
       "         [ 2472,  9042,  6687,     1,    50,  1656],\n",
       "         [    0, 16613,   188,  4401, 12951,   227],\n",
       "         [    2,  3512,  2531,    24,   858,   683],\n",
       "         [    0,  3966, 29999,  1386,  6711,  1058],\n",
       "         [    1,  1174, 27109,     0,   368,  3570],\n",
       "         [29999,   682,    14,   913,    13,  6692],\n",
       "         [  369,  3576,   352,  4350,   555,     2],\n",
       "         [   40,  5052,   527,  6673,    24,  5153],\n",
       "         [    2,     0, 29999,   545, 14599,     9],\n",
       "         [29999,    10,     0,   688,    59,     4],\n",
       "         [ 3025,    77,  3163, 17072,   779,    67],\n",
       "         [ 1250,   669,   342,  5179, 16146,   275],\n",
       "         [ 3286,    91,     4,   551,    87,    80],\n",
       "         [29999,   512, 25743,   737, 19313,    74],\n",
       "         [  113,  2120,   180, 10180,     4,   106],\n",
       "         [  829,    24,  5259,   431, 11868,   279],\n",
       "         [ 1534,     1, 12081,     7,     7,    16],\n",
       "         [ 1280, 12797,     3,    10,  8535,     7],\n",
       "         [  562,  2665,    65,    18,   987,   520],\n",
       "         [29999,  2695,    27,   216,   205,  1991],\n",
       "         [    0,   297,   195,     0,  3244,   119],\n",
       "         [   58,    10,     5,    78,   115, 15303],\n",
       "         [  123,     1,     0, 24362,     2,    93],\n",
       "         [ 8295,    10,    92,    69,  2435,     2],\n",
       "         [    5,  7525,     5, 13849,  1709,   334],\n",
       "         [   31,   807,    85,  1255,     6,  3172],\n",
       "         [ 4649,   124,     2,    30,  6318,   910],\n",
       "         [    0,   126,    48,     0,  7088,   107],\n",
       "         [    2,   274,   881,   147,   291,    28],\n",
       "         [  343,   217, 23773,   143, 22474,  2074],\n",
       "         [    4,   102,  2794,    47,   787,     0],\n",
       "         [    8,    12,     8,  3929,  7501,   945],\n",
       "         [    1,     0,    93,   154,     2,   338],\n",
       "         [  738, 17862,    24, 23345,   345,     1],\n",
       "         [    5,  2040,  2625,     0, 10515,   585],\n",
       "         [    2,  7514, 29999,     0, 16962,   858],\n",
       "         [    4, 18093,    13,   478,     9,    81]]),\n",
       " tensor([[ 1383,  3945,     9,  ..., 27412,  2928,  2178],\n",
       "         [ 7456,   584,    15,  ...,   699,    20,  5641],\n",
       "         [ 4673,  6005, 23118,  ...,     2,  2775,    24],\n",
       "         ...,\n",
       "         [ 7635,   300,  6022,  ...,  1001,  1027, 11695],\n",
       "         [   55,  2103,   196,  ...,   553,    48, 16193],\n",
       "         [ 2861,  1874,    44,  ..., 11183, 12480,   398]])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        initrange = 0.5 / self.embed_size\n",
    "        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        self.in_embed.weight.data.uniform_(-initrange, initrange)\n",
    "        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        self.out_embed.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input_labels, pos_labels, neg_labels):\n",
    "        input_embedding = self.in_embed(input_labels)\n",
    "        pos_embedding = self.out_embed(pos_labels)        \n",
    "        neg_embedding = self.out_embed(neg_labels)        \n",
    "        \n",
    "        input_embedding = input_embedding.unsqueeze(2)\n",
    "        pos_dot = torch.bmm(pos_embedding, input_embedding).squeeze(2)\n",
    "        neg_dot = torch.bmm(neg_embedding, -input_embedding).squeeze(2)        \n",
    "        \n",
    "        log_pos = F.logsigmoid(pos_dot).sum(1)\n",
    "        log_neg = F.logsigmoid(neg_dot).sum(1)\n",
    "        \n",
    "        loss = log_pos + log_neg\n",
    "        return -loss\n",
    "    \n",
    "    def input_embedding(self):\n",
    "        return self.in_embed.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iteration 0 420.04705810546875\n",
      "epoch 0 iteration 10 420.02923583984375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ff27b5104fbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneg_labels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0minput_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mpos_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-4741650cd7a3>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mpos_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_encoded\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpos_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mpos_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_encoded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mneg_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_freqs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mpos_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcenter_word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneg_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = EmbeddingModel(VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "for e in range(NUM_EPOCHS):\n",
    "    for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
    "        input_labels = input_labels.long()\n",
    "        pos_labels = pos_labels.long()        \n",
    "        neg_labels = neg_labels.long()      \n",
    "        if USE_CUDA:\n",
    "            input_labels = input_labels.cuda()\n",
    "            pos_labels = pos_labels.cuda()        \n",
    "            neg_labels = neg_labels.cuda() \n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_labels, pos_labels, neg_labels).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 10 == 0:\n",
    "            print('epoch', e, 'iteration', i, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
